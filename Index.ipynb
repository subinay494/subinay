{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26889500",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventNoList = [\n",
    "          \"001\", # Lilac Wildfire 2017\n",
    "          \"002\", # Cranston Wildfire 2018\n",
    "          \"003\", # Holy Wildfire 2018\n",
    "          \"004\", # Hurricane Florence 2018\n",
    "          \"005\", # 2018 Maryland Flood\n",
    "          \"006\", # Saddleridge Wildfire 2019\n",
    "          \"007\", # Hurricane Laura 2020\n",
    "          \"008\" # Hurricane Sally 2020\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "290e2f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "import pandas as pd\n",
    "import lucene\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.analysis.en import EnglishAnalyzer\n",
    "from org.apache.lucene.index import IndexWriter , IndexWriterConfig\n",
    "from org.apache.lucene.store import SimpleFSDirectory , FSDirectory\n",
    "import org.apache.lucene.document as document \n",
    "from org.apache.lucene.document import Document, Field\n",
    "from java.io import File\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53bb8126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Gets the list of days for a specified event number, e.g. '001'\n",
    "def getDaysForEventNo(eventNo):\n",
    "\n",
    "  # We will download a file containing the day list for an event\n",
    "  url = \"http://trecis.org/CrisisFACTs/CrisisFACTS-\"+eventNo+\".requests.json\"\n",
    "\n",
    "  # Download the list and parse as JSON\n",
    "  dayList = requests.get(url).json()\n",
    "\n",
    "  # Print each day\n",
    "  # Note each day object contains the following fields\n",
    "  #   {\n",
    "  #      \"eventID\" : \"CrisisFACTS-001\",\n",
    "  #      \"requestID\" : \"CrisisFACTS-001-r3\",\n",
    "  #      \"dateString\" : \"2017-12-07\",\n",
    "  #      \"startUnixTimestamp\" : 1512604800,\n",
    "  #      \"endUnixTimestamp\" : 1512691199\n",
    "  #   }\n",
    "\n",
    "  return dayList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "803c6f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<jcc.JCCEnv at 0x7f6105579eb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a1d9315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_end(source):\n",
    "    stop = ( '.','?','!')\n",
    "    flag=0\n",
    "    i = 0\n",
    "    M = max(len(s) for s in stop)\n",
    "    L = len(source)\n",
    "    text=\" \"\n",
    "    while i <= L:\n",
    "        m = M\n",
    "        while m > 0:\n",
    "            chunk = source[i:i + m]\n",
    "            if (chunk in stop) and (i==(L-1)):\n",
    "                flag=1\n",
    "                break\n",
    "            m -= 1\n",
    "        else:\n",
    "            m = 1\n",
    "        i += m\n",
    "    if(flag==0):\n",
    "        text=source+\".\"\n",
    "    else:\n",
    "        text=source\n",
    "    #print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "647474ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preproc(query): ## FOR CONTENT\n",
    "    query=re.sub(r'http\\S+', '',query)\n",
    "    query=query.encode(\"ascii\",\"ignore\")\n",
    "    query=query.decode()\n",
    "    query=query.casefold()\n",
    "    tokens=word_tokenize(query)\n",
    "    #print(tokens)\n",
    "    filtered_text=\"\"\n",
    "    for token in tokens:\n",
    "        #print(\"Token\",token)\n",
    "        if (('http' not in token) and ('@' not in token) and ('*'not in token)):\n",
    "            token=ps.stem(token)\n",
    "            token=token.strip(\"/\")\n",
    "            token=token.replace(\".\",\"\")\n",
    "            token=token.replace(\"?\",\"\")\n",
    "            token=token.replace(\"+\",\"\")\n",
    "            token=token.replace(\"!\",\"\")\n",
    "            token=token.replace(\"(\",\"\")\n",
    "            token=token.replace(\")\",\"\")\n",
    "            filtered_text=filtered_text+\" \"+token\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f91e858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(query): ## FOR FACTTEXT\n",
    "    query=query.encode(\"ascii\",\"ignore\")\n",
    "    query=query.decode()\n",
    "    tokens=word_tokenize(query)\n",
    "    #print(tokens)\n",
    "    filtered_text=\"\"\n",
    "    for token in tokens:\n",
    "        #print(\"Token\",token)\n",
    "        if (('@' not in token) and ('*'not in token)):\n",
    "            token=token.strip(\"/\")\n",
    "            #token=token.replace(\"'\",\"\")\n",
    "            token=token.replace(\"+\",\"\")\n",
    "            token=token.replace(\"(\",\"\")\n",
    "            token=token.replace(\")\",\"\")\n",
    "            filtered_text=filtered_text+\" \"+token\n",
    "    filtered_text=text_end(filtered_text)\n",
    "    #print(filtered_text)\n",
    "    return filtered_text[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bf5ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closeWriter():\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf706ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_void_text(docid,cleanDoc,timestamp,requestId):\n",
    "    doc=document.Document()\n",
    "    #print(docid)\n",
    "    #print(cleanDoc)\n",
    "    doc.add(document.Field(\"Docid\",docid,document.TextField.TYPE_STORED))\n",
    "    doc.add(document.Field(\"Content\",cleanDoc,document.TextField.TYPE_STORED))\n",
    "    doc.add(document.Field(\"Timestamp\",str(timestamp),document.TextField.TYPE_STORED))\n",
    "    doc.add(document.Field(\"RequestId\",requestId,document.TextField.TYPE_STORED))\n",
    "    writer.addDocument(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d98c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index(docid,cleanDoc,timestamp,requestId,facttext):\n",
    "    doc=document.Document()\n",
    "    #print(docid)\n",
    "    #print(cleanDoc)\n",
    "    doc.add(document.Field(\"Docid\",docid,document.TextField.TYPE_STORED))\n",
    "    doc.add(document.Field(\"Content\",cleanDoc,document.TextField.TYPE_STORED))\n",
    "    doc.add(document.Field(\"Timestamp\",str(timestamp),document.TextField.TYPE_STORED))\n",
    "    doc.add(document.Field(\"RequestId\",requestId,document.TextField.TYPE_STORED))\n",
    "    doc.add(document.Field(\"Facttext\",facttext,document.TextField.TYPE_STORED))\n",
    "    writer.addDocument(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63b73683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "   \n",
    "# Input text - to summarize \n",
    "def summary(text,itemsAsDataFrame,request):\n",
    "#     stopWords = set(stopwords.words(\"english\"))\n",
    "#     words = word_tokenize(text)\n",
    "#     i=0\n",
    "   \n",
    "# # Creating a frequency table to keep the \n",
    "# # score of each word\n",
    "   \n",
    "#     freqTable = dict()\n",
    "#     for word in words:\n",
    "#         word = word.lower()\n",
    "#         if word in stopWords:\n",
    "#             continue\n",
    "#         if word in freqTable:\n",
    "#             freqTable[word] += 1\n",
    "#         else:\n",
    "#             freqTable[word] = 1\n",
    "   \n",
    "# Creating a dictionary to keep the score\n",
    "# of each sentence\n",
    "    sentences =text.split(\". \")\n",
    "    sentenceValue = dict()\n",
    "    sentenceId=dict()\n",
    "    sentenceTimestamp=dict()\n",
    "    sentenceOriginal=dict()\n",
    "    #print(sentences)\n",
    "    print(len(sentences))\n",
    "    print((len(itemsAsDataFrame)))\n",
    "    i=0\n",
    "    for sentence in sentences:\n",
    "        sentenceId[sentence]=itemsAsDataFrame['doc_id'][i]\n",
    "        i+=1\n",
    "    i=0\n",
    "    for sentence in sentences:\n",
    "        sentenceTimestamp[sentence]=itemsAsDataFrame['unix_timestamp'][i]\n",
    "        i+=1\n",
    "    i=0\n",
    "    for sentence in sentences:\n",
    "#         print(\"stemed\")\n",
    "#         print(sentence)\n",
    "#         print(\"original\")\n",
    "#         print((itemsAsDataFrame['text'][i]))\n",
    "        sentenceOriginal[sentence]=text_clean(itemsAsDataFrame['text'][i])\n",
    "        i+=1\n",
    "    \n",
    "            \n",
    "    #print(sentenceId)   \n",
    "#     for sentence in sentences:\n",
    "#         for word, freq in freqTable.items():\n",
    "#             if word in sentence.lower():\n",
    "#                 if sentence in sentenceValue:\n",
    "#                     sentenceValue[sentence] += freq\n",
    "#                 else:\n",
    "#                     sentenceValue[sentence] = freq\n",
    "        \n",
    "   \n",
    "   \n",
    "#     #print(len(sentenceValue))\n",
    "#     sumValues = 0\n",
    "#     for sentence in sentenceValue:\n",
    "#         sumValues += sentenceValue[sentence]\n",
    "    \n",
    "   \n",
    "# # Average value of a sentence from the original text\n",
    "    \n",
    "#     average = int(sumValues / len(sentenceValue))\n",
    "# # Storing sentences into our summary.\n",
    "#     summary =[]\n",
    "#     doc_id=[]\n",
    "#     timestamp=[]\n",
    "    for sentence in sentences:\n",
    "        docid=sentenceId[sentence]\n",
    "        content=sentence\n",
    "        timestamp=sentenceTimestamp[sentence]\n",
    "        requestId=request[\"requestID\"]\n",
    "        facttext=sentenceOriginal[sentence]\n",
    "        index(docid,content,timestamp,requestId,facttext)\n",
    "#             doc_id.append(sentenceId[sentence])\n",
    "#             summary.append(sentence)\n",
    "#             timestamp.append(sentenceTimestamp[sentence])\n",
    "        \n",
    "            \n",
    "    return summary,docid,timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09b229f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_end(source):\n",
    "    stop = ( '.')\n",
    "    flag=0\n",
    "    i = 0\n",
    "    M = max(len(s) for s in stop)\n",
    "    L = len(source)\n",
    "    text=\" \"\n",
    "    while i <= L:\n",
    "        m = M\n",
    "        while m > 0:\n",
    "            chunk = source[i:i + m]\n",
    "            if (chunk in stop) and (i==(L-1)):\n",
    "                flag=1\n",
    "                break\n",
    "            m -= 1\n",
    "        else:\n",
    "            m = 1\n",
    "        i += m\n",
    "    if(flag==0):\n",
    "        text=source+\".\"\n",
    "    else:\n",
    "        text=source\n",
    "    #print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36d04f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact_text(itemsAsDataFrame,dataset,request,path):\n",
    "    text=\" \"\n",
    "    for j in range(0,len(itemsAsDataFrame)):\n",
    "        s=text_preproc(itemsAsDataFrame['text'][j])\n",
    "        text=text+\" \" +sentence_end(s)\n",
    "    #print(len(text))\n",
    "#     if len(text)==41:\n",
    "#         for i in range(0,len(itemsAsDataFrame)):\n",
    "#             docid=itemsAsDataFrame['doc_id'][i]\n",
    "#             content=itemsAsDataFrame['text'][i]\n",
    "#             timestamp=itemsAsDataFrame['unix_timestamp'][i]\n",
    "#             requestId=request[\"requestID\"]\n",
    "#             index_void_text(docid,content,timestamp,requestId)\n",
    "#         closeWriter()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        k,l,time=summary(text,itemsAsDataFrame,request)\n",
    "        closeWriter()\n",
    "    #query_retrieval(k,l,dataset,request,time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72ff44f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(event,request,path):\n",
    "    dataset = ir_datasets.load(event)\n",
    "    itemsAsDataFrame = pd.DataFrame(dataset.docs_iter())\n",
    "    fact_text(itemsAsDataFrame,dataset,request,path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3ae31e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 001\n",
      "7288\n",
      "7288\n",
      "##\n",
      "19231\n",
      "19231\n",
      "##\n",
      "5839\n",
      "5839\n",
      "##\n",
      "4407\n",
      "4407\n",
      "##\n",
      "3394\n",
      "3394\n",
      "##\n",
      "2805\n",
      "2805\n",
      "##\n",
      "2658\n",
      "2658\n",
      "##\n",
      "2728\n",
      "2728\n",
      "##\n",
      "2665\n",
      "2665\n",
      "##\n",
      "Event 002\n",
      "5056\n",
      "5056\n",
      "##\n",
      "7866\n",
      "7866\n",
      "##\n",
      "7433\n",
      "7433\n",
      "##\n",
      "5238\n",
      "5238\n",
      "##\n",
      "4691\n",
      "4691\n",
      "##\n",
      "251\n",
      "251\n",
      "##\n",
      "Event 003\n",
      "4402\n",
      "4402\n",
      "##\n",
      "5727\n",
      "5727\n",
      "##\n",
      "5925\n",
      "5925\n",
      "##\n",
      "5984\n",
      "5984\n",
      "##\n",
      "6233\n",
      "6233\n",
      "##\n",
      "4023\n",
      "4023\n",
      "##\n",
      "195\n",
      "195\n",
      "##\n",
      "Event 004\n",
      "7959\n",
      "7959\n",
      "##\n",
      "503\n",
      "503\n",
      "##\n",
      "1559\n",
      "1559\n",
      "##\n",
      "3329\n",
      "3329\n",
      "##\n",
      "6056\n",
      "6056\n",
      "##\n",
      "8668\n",
      "8668\n",
      "##\n",
      "38738\n",
      "38738\n",
      "##\n",
      "55465\n",
      "55465\n",
      "##\n",
      "17000\n",
      "17000\n",
      "##\n",
      "39000\n",
      "39000\n",
      "##\n",
      "58610\n",
      "58610\n",
      "##\n",
      "20301\n",
      "20301\n",
      "##\n",
      "13528\n",
      "13528\n",
      "##\n",
      "18202\n",
      "18202\n",
      "##\n",
      "710\n",
      "710\n",
      "##\n",
      "Event 005\n",
      "7380\n",
      "7380\n",
      "##\n",
      "16407\n",
      "16407\n",
      "##\n",
      "10226\n",
      "10226\n",
      "##\n",
      "7757\n",
      "7757\n",
      "##\n",
      "Event 006\n",
      "6993\n",
      "6993\n",
      "##\n",
      "15014\n",
      "15014\n",
      "##\n",
      "9364\n",
      "9364\n",
      "##\n",
      "6998\n",
      "6998\n",
      "##\n",
      "Event 007\n",
      "46021\n",
      "46021\n",
      "##\n",
      "16161\n",
      "16161\n",
      "##\n",
      "Event 008\n",
      "2215\n",
      "2215\n",
      "##\n",
      "8656\n",
      "8656\n",
      "##\n",
      "7678\n",
      "7678\n",
      "##\n",
      "11000\n",
      "11000\n",
      "##\n",
      "22399\n",
      "22399\n",
      "##\n",
      "10000\n",
      "10000\n",
      "##\n",
      "17087\n",
      "17087\n",
      "##\n",
      "5437\n",
      "5437\n",
      "##\n"
     ]
    }
   ],
   "source": [
    "for eventNo in eventNoList: # for each event\n",
    "    dayList = getDaysForEventNo(eventNo) # get the list of days\n",
    "    print(\"Event \"+eventNo)\n",
    "    #print(dayList[0]['requestID'])\n",
    "    for day in dayList: # for each day\n",
    "        event=\"crisisfacts/\"+eventNo+\"/\"+day[\"dateString\"]\n",
    "        path=\"crisisfacts_\"+str(eventNo)+\"_\"+str(day[\"dateString\"])+\"/\"\n",
    "        indexPath=File(path).toPath()\n",
    "        indexDir=FSDirectory.open(indexPath)\n",
    "        writerConfig=IndexWriterConfig(EnglishAnalyzer())\n",
    "        writer=IndexWriter(indexDir,writerConfig)\n",
    "        data(event,day,path)\n",
    "        print(\"##\")\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38495ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7c9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
